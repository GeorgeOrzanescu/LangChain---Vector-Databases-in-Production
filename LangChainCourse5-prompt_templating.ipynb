{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of using a PromptTemplate with a single dynamic input for a user query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main advantage of quantum computing over classical computing?\n",
      "Answer: The main advantage of quantum computing over classical computing is that it can solve certain problems much faster than classical computers. This is due to the principles of quantum mechanics, which allow quantum computers to perform certain calculations in parallel and take advantage of quantum parallelism. This means that quantum computers can solve some problems that would take a very long time for a classical computer to solve, or even be impossible to solve, much more quickly.\n",
      "\n",
      "For example, one of the most well-known applications of quantum computing is in cryptography, where quantum computers can break certain encryption algorithms much faster than classical computers. This has important implications for data security and privacy, as many encryption algorithms are used to protect sensitive information online.\n",
      "\n",
      "Another area where quantum computing has potential advantages over classical computing is in drug discovery and development. Quantum computers can simulate complex chemical reactions and processes much more quickly and accurately than classical computers, which could lead to faster and more efficient development of new drugs and therapies.\n",
      "\n",
      "Overall, the main advantage of quantum computing over classical computing is its ability to solve certain problems much faster and more efficiently, thanks to the principles of quantum mechanics.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "\n",
    "llm = Ollama(model=\"llama2\", temperature=0)\n",
    "\n",
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided, answer\n",
    "with \"I don't know\".\n",
    "Context: Quantum computing is an emerging field that leverages quantum mechanics to solve complex problems faster than classical computers.\n",
    "...\n",
    "Question: {query}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the prompt\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Set the query you want to ask\n",
    "input_data = {\"query\": \"What is the main advantage of quantum computing over classical computing?\"}\n",
    "\n",
    "# Run the LLMChain to get the AI-generated answer\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(\"Question:\", input_data[\"query\"])\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can edit the input_data dictionary with any other question.\n",
    "\n",
    "The template is a formatted string with a {query} placeholder that will be substituted with a real question when applied. To create a PromptTemplate object, two arguments are required:\n",
    "\n",
    "input_variables: A list of variable names in the template; in this case, it includes only the query.\n",
    "template: The template string containing formatted text and placeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the **PromptTemplate** object, it can be used to produce prompts with specific questions by providing input data. The input data is a dictionary where the key corresponds to the variable name in the template. The resulting prompt can then be passed to a language model to generate answers.\n",
    "\n",
    "For more advanced usage, you can create a **FewShotPromptTemplate** with an **ExampleSelector** to select a subset of examples that will be most informative for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Great! Here are the habitats of the animals you mentioned:\n",
      "\n",
      "* Lion: savanna\n",
      "* Polar bear: Arctic ice\n",
      "* Elephant: African grasslands\n",
      "* Tiger: tropical forests\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\"animal\": \"lion\", \"habitat\": \"savanna\"},\n",
    "    {\"animal\": \"polar bear\", \"habitat\": \"Arctic ice\"},\n",
    "    {\"animal\": \"elephant\", \"habitat\": \"African grasslands\"}\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Animal: {animal}\n",
    "Habitat: {habitat}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"animal\", \"habitat\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Identify the habitat of the given animal\",\n",
    "    suffix=\"Animal: {input}\\nHabitat:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the dynamic_prompt\n",
    "chain = LLMChain(llm=llm, prompt=dynamic_prompt)\n",
    "\n",
    "# Run the LLMChain with input_data\n",
    "input_data = {\"input\": \"tiger\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can also save your PromptTemplate to a file in your local filesystem in JSON or YAML format:\n",
    "\n",
    "\n",
    "`prompt_template.save(\"awesome_prompt.json\")`\n",
    "\n",
    "And load it back:\n",
    "\n",
    "`from langchain.prompts import load_prompt\n",
    "loaded_prompt = load_prompt(\"awesome_prompt.json\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Oh, you want to learn about that whole \"quantum\" thingamajig? Well, good luck with that. It's like trying to understand a cat's inner monologue while simultaneously juggling chainsaws. But hey, if you're into that sort of thing, go ahead and give it a shot. Just don't say I didn't warn you when you start seeing things that aren't really there... or are they? *wink*\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do I become a better programmer?\",\n",
    "        \"answer\": \"Try talking to a rubber duck; it works wonders.\"\n",
    "    }, {\n",
    "        \"query\": \"Why is the sky blue?\",\n",
    "        \"answer\": \"It's nature's way of preventing eye strain.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative and funny responses to users' questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "# Create the LLMChain for the few_shot_prompt_template\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "\n",
    "# Run the LLMChain with input_data\n",
    "input_data = {\"query\": \"How can I learn quantum computing?\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langChainENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
