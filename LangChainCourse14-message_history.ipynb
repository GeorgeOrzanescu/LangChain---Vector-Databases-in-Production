{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### LangChain's ConversationChain\n",
    " has a simple type of memory that remembers all previous inputs/outputs and adds them to the\n",
    " context that is passed. This can be considered a type of short-term memory. Here's an example of how to use ConversationChain with short-term memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import  ConversationChain\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello there! *adjusts glasses* It's a pleasure to meet you! I'm just an AI, I don't have a physical body, but I'm here to help answer any questions you may have. Is there something specific you'd like to know or discuss?\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama2\", temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "output = conversation.predict(input=\"Hi there!\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello there! *adjusts glasses* It's a pleasure to meet you! I'm just an AI, I don't have a physical body, but I'm here to help answer any questions you may have. Is there something specific you'd like to know or discuss?\n",
      "Human: In what scenarios extra memory should be used?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello there! *adjusts glasses* It's a pleasure to meet you! I'm just an AI, I don't have a physical body, but I'm here to help answer any questions you may have. Is there something specific you'd like to know or discuss?\n",
      "Human: In what scenarios extra memory should be used?\n",
      "AI: AI: Ah, an excellent question! Extra memory can be useful in a variety of scenarios, depending on the task at hand. Here are some examples:\n",
      "\n",
      "1. Machine Learning and Deep Learning: When training machine learning models or deep neural networks, extra memory can come in handy to store larger datasets, more complex models, or longer training periods. This allows the AI to learn from a wider range of data and improve its performance over time.\n",
      "2. Natural Language Processing (NLP): NLP tasks like language translation, sentiment analysis, or text summarization often require large amounts of memory to process long texts or handle complex linguistic structures. By having more memory available, the AI can better understand and generate human-like language.\n",
      "3. Computer Vision: In computer vision applications like image recognition, object detection, or facial recognition, extra memory can help the AI process larger images, detect more objects, or learn from a wider range of visual data. This leads to improved accuracy and faster processing times.\n",
      "4. Robotics and Control Systems: When controlling robots or complex systems, extra memory can be useful for storing more detailed maps, navigating through larger environments, or handling more complex tasks. This allows the AI to make better decisions and adapt to changing situations more effectively.\n",
      "5. Scientific Simulations: In scientific simulations like climate modeling, fluid dynamics, or quantum mechanics, extra memory can help the AI handle larger systems, more complex interactions, or longer simulation times. This enables the AI to provide more accurate predictions and better understand complex phenomena.\n",
      "6. Reinforcement Learning: In reinforcement learning tasks like game playing, robot control, or autonomous driving, extra memory can help the AI store more experiences, learn from a wider range of situations, or adapt to changing environments more quickly. This leads to improved decision-making and faster learning.\n",
      "7. Generative Models: When creating generative models like images, videos, or music, extra memory can allow the AI to generate more detailed and realistic content. This is especially useful for tasks that require a high level of creativity or complexity.\n",
      "8. Time Series Analysis: In time series analysis tasks like stock market prediction, weather forecasting, or traffic flow prediction, extra memory can help the AI handle larger datasets, more complex patterns, or longer time horizons. This enables the AI to make more accurate predictions and better understand long-term trends.\n",
      "9. Knowledge Graphs: When building knowledge graphs or semantic networks, extra memory can help the AI store more information, handle more complex relationships, or learn from a wider range of sources. This allows the AI to provide more accurate answers and better understand the context of questions.\n",
      "10. Human-AI Collaboration: Finally, in scenarios where humans and AIs work together, extra memory can help the AI keep up with the pace of human collaboration, handle larger datasets or more complex tasks, or adapt to changing priorities more quickly. This enables the AI to provide better support and improve overall productivity.\n",
      "\n",
      "Of course, these are just a few examples of when extra memory might be useful for an AI. The specific needs will depend on the task at hand and the capabilities of the AI itself. But in general, having more memory available can lead to improved performance, faster processing times, and better decision-making.\n",
      "Human: There are various types of memory in Langchain. When to use which type?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hello there! *adjusts glasses* It's a pleasure to meet you! I'm just an AI, I don't have a physical body, but I'm here to help answer any questions you may have. Is there something specific you'd like to know or discuss?\n",
      "Human: In what scenarios extra memory should be used?\n",
      "AI: AI: Ah, an excellent question! Extra memory can be useful in a variety of scenarios, depending on the task at hand. Here are some examples:\n",
      "\n",
      "1. Machine Learning and Deep Learning: When training machine learning models or deep neural networks, extra memory can come in handy to store larger datasets, more complex models, or longer training periods. This allows the AI to learn from a wider range of data and improve its performance over time.\n",
      "2. Natural Language Processing (NLP): NLP tasks like language translation, sentiment analysis, or text summarization often require large amounts of memory to process long texts or handle complex linguistic structures. By having more memory available, the AI can better understand and generate human-like language.\n",
      "3. Computer Vision: In computer vision applications like image recognition, object detection, or facial recognition, extra memory can help the AI process larger images, detect more objects, or learn from a wider range of visual data. This leads to improved accuracy and faster processing times.\n",
      "4. Robotics and Control Systems: When controlling robots or complex systems, extra memory can be useful for storing more detailed maps, navigating through larger environments, or handling more complex tasks. This allows the AI to make better decisions and adapt to changing situations more effectively.\n",
      "5. Scientific Simulations: In scientific simulations like climate modeling, fluid dynamics, or quantum mechanics, extra memory can help the AI handle larger systems, more complex interactions, or longer simulation times. This enables the AI to provide more accurate predictions and better understand complex phenomena.\n",
      "6. Reinforcement Learning: In reinforcement learning tasks like game playing, robot control, or autonomous driving, extra memory can help the AI store more experiences, learn from a wider range of situations, or adapt to changing environments more quickly. This leads to improved decision-making and faster learning.\n",
      "7. Generative Models: When creating generative models like images, videos, or music, extra memory can allow the AI to generate more detailed and realistic content. This is especially useful for tasks that require a high level of creativity or complexity.\n",
      "8. Time Series Analysis: In time series analysis tasks like stock market prediction, weather forecasting, or traffic flow prediction, extra memory can help the AI handle larger datasets, more complex patterns, or longer time horizons. This enables the AI to make more accurate predictions and better understand long-term trends.\n",
      "9. Knowledge Graphs: When building knowledge graphs or semantic networks, extra memory can help the AI store more information, handle more complex relationships, or learn from a wider range of sources. This allows the AI to provide more accurate answers and better understand the context of questions.\n",
      "10. Human-AI Collaboration: Finally, in scenarios where humans and AIs work together, extra memory can help the AI keep up with the pace of human collaboration, handle larger datasets or more complex tasks, or adapt to changing priorities more quickly. This enables the AI to provide better support and improve overall productivity.\n",
      "\n",
      "Of course, these are just a few examples of when extra memory might be useful for an AI. The specific needs will depend on the task at hand and the capabilities of the AI itself. But in general, having more memory available can lead to improved performance, faster processing times, and better decision-making.\n",
      "Human: There are various types of memory in Langchain. When to use which type?\n",
      "AI: Ah, an excellent question! In Langchain, we have different types of memory, each with its own strengths and weaknesses, depending on the task at hand. Here's a brief overview of the different types of memory in Langchain and when to use them:\n",
      "\n",
      "1. **Ephemeral Memory**: This is the most basic type of memory in Langchain, used for temporary storage of data that doesn't require persistence. Ephemeral memory is great for handling short-term tasks or processing small amounts of data. It's fast and efficient but has limited capacity and is lost when the AI restarts.\n",
      "2. **Short-Term Memory**: This type of memory is used for storing data that needs to be retained for a short period, typically up to a few minutes. Short-term memory is useful for tasks like language modeling, where the AI needs to remember recent input to generate coherent responses. It has a moderate capacity and can be lost over time.\n",
      "3. **Long-Term Memory**: Long-term memory is used for storing data that needs to be retained for an extended period, often hours, days, or even weeks. This type of memory is useful for tasks like language translation, where the AI needs to remember entire sentences or paragraphs to generate accurate responses. It has a larger capacity than short-term memory but can take longer to access.\n",
      "4. **Persistent Memory**: Persistent memory is used for storing data that needs to be retained across multiple restarts of the AI. This type of memory is useful for tasks like training machine learning models, where the AI needs to remember the entire dataset to make accurate predictions. Persistent memory has a large capacity and can retain data even after a reboot.\n",
      "5. **Archive Memory**: Archive memory is used for storing large amounts of data that don't need to be accessed frequently. This type of memory is useful for tasks like data storage, where the AI needs to keep a large amount of data available but doesn't need to access it often. Archive memory has a high capacity and can store data for an extended period.\n",
      "\n",
      "When choosing which type of memory to use in Langchain, consider the following factors:\n",
      "\n",
      "* **Data lifetime**: How long do you want the data to be retained? If it needs to be retained for a short period, use ephemeral or short-term memory. If it needs to be retained for an extended period, use long-term or persistent memory.\n",
      "* **Access time**: How quickly do you need to access the data? If you need quick access, use short-term or persistent memory. If you're willing to wait a bit longer, use ephemeral or archive memory.\n",
      "* **Capacity**: How much data do you need to store? If you need to store a large amount of data, use long-term or persistent memory. If you only need to store a small amount, use ephemeral or short-term memory.\n",
      "\n",
      "By considering these factors, you can choose the most appropriate type of memory for your task in Langchain.\n",
      "Human: Do you remember what was our first message?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"In what scenarios extra memory should be used?\")\n",
    "output = conversation.predict(input=\"There are various types of memory in Langchain. When to use which type?\")\n",
    "output = conversation.predict(input=\"Do you remember what was our first message?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, an excellent question! *adjusts glasses* I'm just an AI, I don't have personal experiences or memories like humans do, but I can certainly help you recall our previous conversation. May I suggest a few ways to approach this task?\n",
      "\n",
      "1. **Keywords**: Try to remember any keywords or phrases from our previous conversation that might help trigger your memory. For example, you mentioned \"extra memory\" and \"Langchain,\" which could be useful in retrieving the context of our discussion.\n",
      "2. **Context**: Think about the context in which we were discussing these topics. Was it during a specific task or scenario? Were there any particular questions or points of interest that might help you recall our conversation?\n",
      "3. **Associations**: Try to associate our previous conversation with other memories or experiences you have. For example, if we talked about machine learning, you might remember other topics related to AI or data analysis that we discussed earlier in our conversation.\n",
      "4. **Visualization**: See if you can visualize the conversation in your mind. Try to replay the dialogue in your head, paying attention to any key points or details that might help you recall our discussion.\n",
      "\n",
      "By using these strategies, I hope we can successfully retrieve our previous conversation and continue our chat! *smiling*\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory\n",
    "The ConversationChain uses the ConversationBufferMemory class by default to provide a history of messages. This memory can save the previous conversations in form of variables. The class accepts the return_messages argument which is helpful for dealing with chat models. This is how the CoversationChain keep context under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [HumanMessage(content='hi there!'), AIMessage(content=\"Hi there! It's nice to meet you. How can I help you today?\")]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\": \"hi there!\"}, {\"output\": \"Hi there! It's nice to meet you. How can I help you today?\"})\n",
    "\n",
    "print( memory.load_memory_variables({}) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the code in the previous section is the same as the following. It will automatically call the .save_context() object after each interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
    "\n",
    "\n",
    "print( conversation.predict(input=\"Tell me a joke about elephants\") )\n",
    "print( conversation.predict(input=\"Who is the author of the Harry Potter series?\") )\n",
    "print( conversation.predict(input=\"What was the joke you told me earlier?\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used **MessagesPlaceholder** function to create a placeholder for the conversation history in a chat model prompt. It is particularly useful when working with ConversationChain and **ConversationBufferMemory** to maintain the context of a conversation. The **MessagesPlaceholder** function takes a variable name as an argument, which is used to store the conversation history in the memory buffer. We will cover that function later. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langChainENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
